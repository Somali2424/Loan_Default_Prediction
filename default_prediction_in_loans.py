# -*- coding: utf-8 -*-
"""Default Prediction in Loans.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1limAI8txUXruNjSgXyuBlFd-MZoZvlq4
"""

import pandas as pd, numpy as np, plotly.express as px, matplotlib.pyplot as plt, seaborn as sns

from plotly.offline import init_notebook_mode
init_notebook_mode(connected=True)

"""# EXPLORATORY DATA ANALYSIS"""

df=pd.read_csv('Default_Fin.csv')
df

"""We've got a dataset with 10,000 pieces of information that can help us determine if a person is likely to fail to repay a loan or not. This dataset includes four key factors: employment status, bank balance when the data was collected, annual salary, and a binary indicator for loan default (0 for 'defaulted' and 1 for 'not defaulted'). This information is crucial for a financial organization when deciding whether to grant a loan to an individual. That's why this project is so important

# New Section
"""

from google.colab import drive
drive.mount('/content/drive')

#Checking if there is any null value
df.isnull().sum()

#Renaming target variable
df.rename({'Defaulted?':'Defaulted'},axis=1,inplace=True)
df

Condition = ['Defaulted', 'Did not default']

data = df['Defaulted'].value_counts()

# Creating color parameters
colors = ( "blue", "orange")

# Wedge properties
wp = { 'linewidth' : 1, 'edgecolor' : "green" }

# Creating autocpt arguments
def func(pct, allvalues):
    absolute = int(pct / 100.*np.sum(allvalues))
    return "{:.1f}%\n({:d} clients)".format(pct, absolute)

# Creating plot
fig, ax = plt.subplots(figsize =(10, 7))
wedges, texts, autotexts = ax.pie(data,
                                  autopct = lambda pct: func(pct, data),
                                  labels = Condition,
                                  shadow = True,
                                  colors = colors,
                                  startangle = 90,
                                  wedgeprops = wp,
                                  textprops = dict(color ="black"))

# Adding legend
ax.legend(wedges, Condition,
          title ="Defaulted_Condition",
          loc ="center left",
          bbox_to_anchor =(1, 0, 0.5, 1))

plt.setp(autotexts, size = 8, weight ="bold")
ax.set_title("Distribution of Clients who have Defaulted")

# show plot
plt.show()

"""It is possible to see most of the clients did not default on their loans.This means we are going have a class imbalance issue. We will resolve the issue while building the predictor model with AutoML

Let's separate our clients among those who are employed and those who are unemployed and see how the target variable is distributed among each separate group
"""

#Separating into two groups: Employed clients and unemployed clients

employed = df.query("Employed == 1")
unemployed = df.query("Employed == 0")

"""Then after separating the groups we are again seeing the class balance between two groups. First the class balance is seen within the employed individual and then in the unemployed individuals. Running the above code divides the entire dataset into two parts.. two new dataframes are created, one is named 'employed' dataframe and other is named 'unemployed' dataframe. In unemployed dataframe the 'employed' column only contains value 0....The colummn name itself doesnot changed. Unemployed is only the name of the dataframe."""

unemployed

employed

"""For employed individuals the class balance is:

"""

Condition = ['Defaulted', 'Did not default']

data = employed['Defaulted'].value_counts()

# Creating color parameters
colors = ( "yellow", "orange")

# Wedge properties
wp = { 'linewidth' : 1, 'edgecolor' : "green" }

# Creating autocpt arguments
def func(pct, allvalues):
    absolute = int(pct / 100.*np.sum(allvalues))
    return "{:.1f}%\n({:d} clients)".format(pct, absolute)

# Creating plot
fig, ax = plt.subplots(figsize =(10, 7))
wedges, texts, autotexts = ax.pie(data,
                                  autopct = lambda pct: func(pct, data),
                                  labels = Condition,
                                  shadow = True,
                                  colors = colors,
                                  startangle = 90,
                                  wedgeprops = wp,
                                  textprops = dict(color ="black"))

# Adding legend
ax.legend(wedges, Condition,
          title ="Defaulted_Condition",
          loc ="center left",
          bbox_to_anchor =(1, 0, 0.5, 1))

plt.setp(autotexts, size = 8, weight ="bold")
ax.set_title("Distribution of Clients who have Defaulted")

# show plot
plt.show()

"""For unemployed individuals the class balance is:"""

Condition = ['Defaulted', 'Did not default']

data = unemployed['Defaulted'].value_counts()

# Creating color parameters
colors = ( "pink", "orange")

# Wedge properties
wp = { 'linewidth' : 1, 'edgecolor' : "green" }

# Creating autocpt arguments
def func(pct, allvalues):
    absolute = int(pct / 100.*np.sum(allvalues))
    return "{:.1f}%\n({:d} clients)".format(pct, absolute)

# Creating plot
fig, ax = plt.subplots(figsize =(10, 7))
wedges, texts, autotexts = ax.pie(data,
                                  autopct = lambda pct: func(pct, data),
                                  labels = Condition,
                                  shadow = True,
                                  colors = colors,
                                  startangle = 90,
                                  wedgeprops = wp,
                                  textprops = dict(color ="black"))

# Adding legend
ax.legend(wedges, Condition,
          title ="Defaulted_Condition",
          loc ="center left",
          bbox_to_anchor =(1, 0, 0.5, 1))

plt.setp(autotexts, size = 8, weight ="bold")
ax.set_title("Distribution of Clients who have Defaulted")

# show plot
plt.show()

# The unemployed people have the tendency to default more than their employed counterpart

#Default distribution according to bank balance values
fig = plt.figure(figsize = (20, 9))
sns.set_style("dark")
sns.kdeplot(df.query('Defaulted==1')['Bank Balance'])
sns.kdeplot(df.query('Defaulted==0')['Bank Balance'])
plt.title('Default x Bank Balance')
plt.legend(labels=['Defaulted', 'Did Not Default'])
plt.show()

# Default distribution according to annual salaries
fig = plt.figure(figsize = (20, 9))
sns.set_style("dark")
sns.kdeplot(df[df['Defaulted']==1]['Annual Salary'])
sns.kdeplot(df[df['Defaulted']==0]['Annual Salary'])
plt.title('Default x Annual Salaries')
plt.legend(labels=['Defaulted', 'Did Not Default'])
plt.show()

"""There is a threshold below which the higher annual income group are more likely to default and above that threshold the higher income group are more likely to default"""

corr = df.corr()
plt.figure(figsize = (16, 12))
g = sns.heatmap(corr, annot = True)

"""we don't really have any strong correlation between the target variable and any other attribute. It may be because the correlation only shows the linear relationship among variables. Any other relationships are failed to capture by it

# Building a Model to Predict Loan Default

Here we are going to use is the recall score which is going to tell us how good our model can predict the class that it wants to predict..
Our model should be able to predict the customers who are more likely to predict..We want to keep our false negatives as low as possible. false negative implies the defaulters who had been tagged as defaulters by the model and more likely to increase financial loss in the institution

Lets divide our dataset into a Training Set and Testing set
"""

test=df.tail(2000)
#20% of the dataset will be used for testing
test

#Removing  testing data from the dataframe and setting up 80% of data left for training and validation

train=df.drop(test.index)
train

"""Importing PyCaret's classification lib"""

from pycaret.classification import *

models()

s = setup(train, target = 'Defaulted', train_size = 0.75,ignore_features = ['Index'],fix_imbalance = True,normalize = True)

# Let's run a bunch of different classification algorithms and rank them by their recall score
best = compare_models()

"""there is a high cost associated with False Negative. So we sort our models based on the recall score..
We can see 3 models are showing high accuracy and recall score that is Dummy Classifier,ridge Classifier, Linear Discriminant Analysis



"""

from sklearn.linear_model import RidgeClassifier

RidgeClassifier(alpha=1.0, class_weight=None, copy_X=True, fit_intercept=True,
                max_iter=None, random_state=1350,
                solver='auto', tol=0.001)

ridge = create_model('ridge')

lda = create_model('lda')

qda = create_model('qda')

# Blending models
blended_model = blend_models(estimator_list = [ridge, lda, qda],
                            fold = 10,
                            optimize = 'Recall',
                            choose_better = True)

"""No improvement made. Let's tune each one of our models.


"""

tuned_ridge = tune_model(ridge,
                        n_iter = 1000,
                        optimize = 'Recall',
                        choose_better = True)

tuned_lda = tune_model(lda,
                        n_iter = 1000,
                        optimize = 'Recall',
                        choose_better = True)

tuned_qda = tune_model(qda,
                        n_iter = 1000,
                        optimize = 'Recall',
                        choose_better = True)

"""All three models have almost the same recall score of 94..Tunedridge  has the best accuracy score 85%"""

predict_model(tuned_qda)
#Finally, let's use our model on the hold-out sample to see how well it performs

predict_model(tuned_ridge)

predict_model(tuned_lda)

#Both Ridge Classifier and Linear Discriminant Analysis has the same accuracy level and Recall Value but Linear Discriminant Analysis has a better discriminatory abilities as per the AUC values

#Finalizing model before testing it with unseen data
model = finalize_model(tuned_lda)
print(model) # Printing final model

lda_classifier = LinearDiscriminantAnalysis(
    priors=None,
    store_covariance=False  # Set to True or False as per your requirement
)

test # testing set

predictions = predict_model(model, data = test)

predictions

y_test = predictions['Defaulted']
pred = predictions['prediction_label']

y_test.value_counts()

from sklearn.metrics import confusion_matrix
cm = confusion_matrix(y_test, pred)
plt.figure(figsize=(12,9))
ax = plt.subplot()
sns.heatmap(cm,annot = True, fmt ='g', ax = ax)
ax.set_xlabel('Predicted Class')
ax.set_ylabel('True Class')
ax.set_title('Confusion Matrix')
ax.xaxis.set_ticklabels(['Non-Defaulter','Defaulter'])
ax.yaxis.set_ticklabels(['Non-Defaulter','Defaulter'])
plt.show()

"""Considering that labeling a defaulter as a non-defaulter represents a high chance of financial loss to the institution, the goal of this model was to have the highest recall possible and correctly predict the highest amount of defaulters as possible.

Through an AutoML library called PyCaret, we could successfully run 16 different classification algorithms and find the one with the best recall performance. After tuning it and testing it, we've achieved a 89.55% recall score on unseen data and correctly predicted 61 defaulters among 60 of them.



"""

